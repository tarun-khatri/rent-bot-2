"""
Simplified Gemini AI service for real estate conversations.
Ultra-short, direct responses with minimal fluff.
Fast responses using gemini-2.5-flash-lite model.
"""

import logging
from flask import current_app
from google import genai
from google.genai import types
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


class GeminiServiceSimple:
    """Simplified service for generating ultra-short, direct responses"""
    
    def __init__(self):
        """Initialize Gemini service with fast model"""
        self.client = None
        self.model_name = None
        self._initialized = False
    
    def _initialize_model(self):
        """Initialize the Gemini model with fast settings"""
        try:
            # Initialize the new Gemini client
            self.client = genai.Client(api_key=current_app.config['GEMINI_API_KEY'])
            
            # Use flash-lite model for speed
            self.model_name = current_app.config.get('GEMINI_MODEL', 'gemini-2.5-flash-lite')
            
            self._initialized = True
            logger.info(f"Gemini AI model '{self.model_name}' initialized successfully (simple mode)")
            
        except Exception as e:
            logger.error(f"Failed to initialize Gemini model: {str(e)}")
            raise
    
    def _ensure_initialized(self):
        """Ensure the model is initialized"""
        if not self._initialized:
            self._initialize_model()
    
    def generate_response(self, stage: str, lead_data: Dict[str, Any], conversation_history: List[Dict], user_message: str) -> str:
        """
        Generate ultra-short, direct AI response
        
        Args:
            stage: Current lead stage
            lead_data: Lead information
            conversation_history: Recent conversation history
            user_message: Current user message
            
        Returns:
            str: Ultra-short response in Hebrew (2-3 sentences max)
        """
        self._ensure_initialized()
        
        try:
            logger.info(f"Generating simple response for lead {lead_data.get('id')} in stage: {stage}")
            
            # Build ultra-short prompt
            prompt = self._build_simple_prompt(stage, lead_data, conversation_history, user_message)
            
            # Try simple API call first (like working version)
            try:
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt
                )
            except Exception as e:
                logger.warning(f"Simple API call failed, trying with config: {e}")
                # Fallback with config
                generation_config = types.GenerateContentConfig(
                    temperature=0.8,
                    max_output_tokens=600  # Increased even more
                )
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=generation_config
                )
            
            # Extract response text using proper Gemini API structure
            response_text = ""
            try:
                # Method 1: Direct text attribute (newer API)
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                else:
                    # Method 2: Candidates structure (older API)
                    if hasattr(response, 'candidates') and response.candidates:
                        for i, candidate in enumerate(response.candidates):
                            # Check if candidate has finish_reason MAX_TOKENS
                            if hasattr(candidate, 'finish_reason') and candidate.finish_reason:
                                if 'MAX_TOKENS' in str(candidate.finish_reason):
                                    logger.warning("Response was cut off due to token limit")
                                    response_text = "××” ×”×©× ×©×œ×š?"  # Simple fallback
                                    break
                            
                            if hasattr(candidate, 'content') and candidate.content:
                                content = candidate.content
                                if hasattr(content, 'parts') and content.parts:
                                    for j, part in enumerate(content.parts):
                                        if hasattr(part, 'text') and part.text:
                                            response_text += part.text
                        response_text = response_text.strip()
                
                # If still empty, provide fallback
                if not response_text:
                    logger.warning(f"Empty response from {self.model_name}, using fallback")
                    response_text = "××” ×”×©× ×©×œ×š?"
                    
            except Exception as e:
                logger.error(f"Error extracting response from {self.model_name}: {e}")
                response_text = "××” ×”×©× ×©×œ×š?"
            
            # Clean response - remove any prompt artifacts
            response_text = self._clean_response(response_text)
            
            # Format for WhatsApp
            response_text = self._format_for_whatsapp(response_text)
            
            logger.info(f"Simple response generated for lead {lead_data.get('id')}, length: {len(response_text)}")
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating simple response: {str(e)}")
            return "××¦×˜×¢×¨, ×™×© ×œ×™ ×‘×¢×™×” ×˜×›× ×™×ª. × ×¡×” ×©×•×‘."
    
    def _build_simple_prompt(self, stage: str, lead_data: Dict, conversation_history: List[Dict], user_message: str) -> str:
        """Build prompt with proper context like working version"""
        
        # Base prompt
        prompt = "××ª×” ×¡×•×›×Ÿ × ×“×œ\"×Ÿ. ×¢× ×” ×‘×¢×‘×¨×™×ª ×§×¦×¨ (1-2 ××©×¤×˜×™×).\n\n"
        
        # Add conversation history (like working version)
        if conversation_history:
            recent_messages = conversation_history[-5:]  # Last 5 messages for context
            if recent_messages:
                prompt += "×”×©×™×—×” ×¢×“ ×¢×›×©×™×•:\n"
                for msg in recent_messages:
                    sender = "×‘×•×˜" if msg['message_type'] == 'bot' else "×œ×§×•×—"
                    prompt += f"{sender}: {msg['content']}\n"
                prompt += "\n"
        
        # What we know about the lead (like working version)
        context_parts = []
        if lead_data.get('name'):
            context_parts.append(f"×©×: {lead_data['name']}")
        if lead_data.get('preferred_area'):
            context_parts.append(f"×¤×¨×•×™×§×˜: {lead_data['preferred_area']}")
        if lead_data.get('rooms'):
            context_parts.append(f"×—×“×¨×™×: {lead_data['rooms']}")
        if lead_data.get('budget'):
            context_parts.append(f"×ª×§×¦×™×‘: {lead_data['budget']}")
        
        if context_parts:
            prompt += "××” ×©××ª×” ×™×•×“×¢:\n" + "\n".join(context_parts) + "\n\n"
        
        # Current message
        prompt += f"×”×œ×§×•×— ×××¨: {user_message}\n\n"
        
        # Task based on stage - DON'T use client name in response
        if stage == 'new':
            prompt += "×©××œ: ××” ×”×©× ×©×œ×š? (××œ ×ª×©×ª××© ×‘×©× ×‘×ª×©×•×‘×”)"
        elif stage == 'collecting_profile':
            if not lead_data.get('name'):
                prompt += "×©××œ: ××” ×”×©× ×©×œ×š?"
            elif not lead_data.get('preferred_area'):
                # Include available properties in the prompt
                if lead_data.get('available_properties'):
                    properties_list = ', '.join(lead_data['available_properties'])
                    prompt += f"×©××œ: ×‘××™×–×” ×¤×¨×•×™×§×˜ ××ª×¢× ×™×™×Ÿ? (××œ ×ª×©×ª××© ×‘×©×)\n\n"
                    prompt += f"×—×©×•×‘: ×”×–×›×¨ ×¨×§ ××ª ×”×©××•×ª ×”××œ×” ×‘×“×™×•×§ ×›××• ×©×”×: {properties_list}\n××œ ×ª××¦×™× ×©××•×ª ××—×¨×™×!"
                else:
                    prompt += "×©××œ: ×‘××™×–×” ×¤×¨×•×™×§×˜ ××ª×¢× ×™×™×Ÿ? (××œ ×ª×©×ª××© ×‘×©×)"
            elif not lead_data.get('rooms'):
                prompt += "×©××œ: ×›××” ×—×“×¨×™×? (××œ ×ª×©×ª××© ×‘×©×)"
            else:
                prompt += "×××•×¨: ××—×¤×© ×“×™×¨×•×ª (××œ ×ª×©×ª××© ×‘×©×)"
        elif stage == 'qualified':
            prompt += "×©××œ: ×¨×•×¦×” ×œ×ª×× ×‘×™×§×•×¨? (××œ ×ª×©×ª××© ×‘×©×)"
        else:
            prompt += "×¢×–×•×¨ ×‘×§×¦×¨×”"
        
        return prompt
    
    def _build_context(self, lead_data: Dict) -> str:
        """Build ultra-short context about lead"""
        parts = []
        
        if lead_data.get('name'):
            parts.append(f"×©×: {lead_data['name']}")
        if lead_data.get('preferred_area'):
            parts.append(f"×¤×¨×•×™×§×˜: {lead_data['preferred_area']}")
        if lead_data.get('rooms'):
            parts.append(f"×—×“×¨×™×: {lead_data['rooms']}")
        if lead_data.get('budget'):
            parts.append(f"×ª×§×¦×™×‘: {lead_data['budget']:,.0f}â‚ª")
        if lead_data.get('has_parking') is not None:
            parts.append(f"×—× ×™×”: {'×›×Ÿ' if lead_data['has_parking'] else '×œ×'}")
        
        return "\n".join([f"- {p}" for p in parts]) if parts else ""
    
    def _get_stage_guidance(self, stage: str, lead_data: Dict) -> str:
        """Get ultra-short stage guidance"""
        
        guidance = {
            'new': """
ğŸ¯ ×œ×§×•×— ×—×“×© - ×§×‘×œ ××•×ª×• ×‘×§×¦×¨×” ×•×©××œ ××ª ×”×©× ×©×œ×•.""",
            
            'collecting_profile': f"""
ğŸ¯ ×¦×¨×™×š ×œ××¡×•×£:
{self._get_missing_fields(lead_data)}
×©××œ ×¨×§ ×¢×œ ×©×“×” ××—×“ ×‘×›×œ ×¤×¢×.""",
            
            'qualified': """
ğŸ¯ ×™×© ××ª ×›×œ ×”××™×“×¢ - ×”×¦×¢ ×œ×• ×œ×¨××•×ª ×“×™×¨×•×ª ×–××™× ×•×ª.""",
            
            'showing_properties': """
ğŸ¯ ×ª×¦×™×’ ×“×™×¨×•×ª - ××—×¨×™ ×–×” ×ª×©××œ ×× ×¨×•×¦×” ×œ×ª×× ×‘×™×§×•×¨.""",
            
            'scheduling_in_progress': """
ğŸ¯ ×©×œ×—×ª ×§×™×©×•×¨ ×œ×§×œ× ×“×œ×™ - ×—×›×” ×œ××™×©×•×¨ ×©×§×‘×¢.""",
            
            'tour_scheduled': """
ğŸ¯ ×”×¤×’×™×©×” ×§×‘×•×¢×” - ×¢× ×” ×¨×§ ×¢×œ ×©××œ×•×ª ×¢×œ ×”×¤×’×™×©×”.""",
            
            'asking_guarantees': """
ğŸ¯ ×”×¡×‘×¨ ×¢×œ ×“×¨×™×©×•×ª ×”×¢×¨×‘×•×™×•×ª ×‘×§×¦×¨×”:
- ×ª×œ×•×©×™ ×©×›×¨ ×-2 ×—×•×“×©×™× ××—×¨×•× ×™×
- ×¢×¨×‘×•×ª ×©×œ 2 ×—×•×“×©×™ ×©×›×™×¨×•×ª ××¨××©
×× ××¡×›×™× - ×ª×©×œ×— ×§×œ× ×“×œ×™."""
        }
        
        return guidance.get(stage, f"ğŸ¯ ×©×œ×‘ {stage} - ×¢× ×” ×‘×§×¦×¨×” ×•×œ×¢× ×™×™×Ÿ")
    
    def _get_missing_fields(self, lead_data: Dict) -> str:
        """Get list of missing fields"""
        missing = []
        if not lead_data.get('preferred_area'):
            missing.append("- ××™×–×” ×¤×¨×•×™×§×˜? (Sderot Yerushalayim / Neve Sharet / Afar House)")
        if not lead_data.get('rooms'):
            missing.append("- ×›××” ×—×“×¨×™×?")
        
        return "\n".join(missing) if missing else "- ×”×›×œ ×™×©!"
    
    def _format_for_whatsapp(self, response: str) -> str:
        """Format response for WhatsApp - minimal changes"""
        # Convert ** to * for WhatsApp bold
        response = response.replace("**", "*")
        
        # Remove AI artifacts
        response = response.replace("×‘×•×˜", "").replace("AI", "")
        
        # Ensure not too long
        if len(response) > 500:
            response = response[:497] + "..."
        
        return response.strip()
    
    def _clean_response(self, response: str) -> str:
        """Clean response to remove prompt artifacts"""
        if not response:
            return response
            
        # Remove common prompt artifacts
        lines = response.split('\n')
        cleaned_lines = []
        
        for line in lines:
            line = line.strip()
            # Skip lines that look like prompt data
            if (line.startswith('×©×:') and '×”×œ×§×•×—:' in line) or \
               line.startswith('××” ×©××ª×” ×™×•×“×¢:') or \
               line.startswith('×”×©×™×—×” ×¢×“ ×¢×›×©×™×•:') or \
               line.startswith('×”×œ×§×•×— ×××¨:') or \
               line.startswith('×©××œ:') or \
               line.startswith('×××•×¨:'):
                continue
            # Keep actual response lines
            if line and not line.startswith('×©×:') and not line.startswith('×”×œ×§×•×—:'):
                cleaned_lines.append(line)
        
        # If we have cleaned lines, use them
        if cleaned_lines:
            return '\n'.join(cleaned_lines).strip()
        
        # Fallback to original response
        return response.strip()
    
    def generate_property_message(self, lead_data: Dict, properties: List[Dict]) -> str:
        """Generate ultra-short property recommendation"""
        self._ensure_initialized()
        
        try:
            # Ultra simple prompt
            prop = properties[0]
            prop_info = prop.get('properties', {}) if isinstance(prop.get('properties'), dict) else {}
            prompt = f"""×¡×•×›×Ÿ × ×“×œ"×Ÿ. ×¢× ×” ×‘×¢×‘×¨×™×ª ×§×¦×¨.

××¦××ª×™: {prop.get('rooms')} ×—×“×¨×™× ×‘-{lead_data.get('preferred_area', '')} - {prop.get('price'):,.0f}â‚ª/×—×•×“×©

×××•×¨: ×™×© ×“×™×¨×•×ª. ×©×•×œ×— ×ª××•× ×•×ª. ×¨×•×¦×” ×¤×¨×˜×™× ××• ×œ×ª×× ×‘×™×§×•×¨?"""
            
            generation_config = types.GenerateContentConfig(
                temperature=0.8,
                max_output_tokens=300  # Increased - was cutting off responses
            )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generation_config
            )
            
            # Extract text with proper error handling (same as old working version)
            response_text = ""
            try:
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text += part.text
                
                response_text = response_text.strip()
                
                if not response_text:
                    logger.warning(f"Empty property message from {self.model_name}")
                    response_text = f"××¦××ª×™ {len(properties)} ×“×™×¨×•×ª ××ª××™××•×ª. ×©×•×œ×— ×ª××•× ×•×ª..."
                    
            except Exception as e:
                logger.error(f"Error extracting property message from {self.model_name}: {e}")
                response_text = f"××¦××ª×™ {len(properties)} ×“×™×¨×•×ª ××ª××™××•×ª. ×©×•×œ×— ×ª××•× ×•×ª..."
            
            return self._format_for_whatsapp(response_text)
            
        except Exception as e:
            logger.error(f"Error generating property message: {e}")
            return f"××¦××ª×™ {len(properties)} ×“×™×¨×•×ª ××ª××™××•×ª. ×©×•×œ×— ×ª××•× ×•×ª..."
    
    def generate_no_properties_message(self, lead_data: Dict) -> str:
        """Generate ultra-short message when no properties found"""
        self._ensure_initialized()
        
        try:
            # Ultra simple prompt
            prompt = f"""×¡×•×›×Ÿ × ×“×œ"×Ÿ. ×¢× ×” ×‘×¢×‘×¨×™×ª ×§×¦×¨.

×—×™×¤×©: {lead_data.get('rooms')} ×—×“×¨×™× ×‘-{lead_data.get('preferred_area', '')}

×××•×¨: ×œ× ××¦××ª×™. ×’××™×© ×‘×¤×¨×•×™×§×˜ ××• ×—×“×¨×™×?"""
            
            generation_config = types.GenerateContentConfig(
                temperature=0.8,
                max_output_tokens=250  # Increased - was cutting off responses
            )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generation_config
            )
            
            # Extract text with proper error handling (same as old working version)
            response_text = ""
            try:
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and candidate.content:
                            content = candidate.content
                            if hasattr(content, 'parts') and content.parts:
                                for part in content.parts:
                                    if hasattr(part, 'text') and part.text:
                                        response_text += part.text
                
                response_text = response_text.strip()
                
                if not response_text:
                    logger.warning(f"Empty no properties message from {self.model_name}, using AI to generate")
                    # Generate a simple fallback using AI
                    response_text = self.generate_response('collecting_profile', lead_data, [], f"×œ× ××¦××ª×™ ×“×™×¨×•×ª ×¢× {lead_data.get('rooms')} ×—×“×¨×™× ×‘-{lead_data.get('preferred_area')}. ×’××™×©?")
                    
            except Exception as e:
                logger.error(f"Error extracting no properties message from {self.model_name}: {e}")
                # Generate a simple fallback using AI
                response_text = self.generate_response('collecting_profile', lead_data, [], f"×œ× ××¦××ª×™ ×“×™×¨×•×ª ×¢× {lead_data.get('rooms')} ×—×“×¨×™× ×‘-{lead_data.get('preferred_area')}. ×’××™×©?")
            
            response_text = self._clean_response(response_text)
            
            return self._format_for_whatsapp(response_text)
            
        except Exception as e:
            logger.error(f"Error generating no properties message: {e}")
            return "×œ× ××¦××ª×™ ×‘×“×™×•×§ ××” ×©×—×™×¤×©×ª. ×’××™×© ×‘×¤×¨×•×™×§×˜ ××• ×‘××¡×¤×¨ ×—×“×¨×™×?"


# Global instance
gemini_service_simple = GeminiServiceSimple()


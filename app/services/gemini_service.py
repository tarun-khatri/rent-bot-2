"""
Human-like Gemini AI service for real estate conversations.
Generates natural, personalized responses with emojis and proper WhatsApp formatting.
Maintains business flow while feeling completely human and conversational.
"""

import logging
from flask import current_app
from google import genai
from google.genai import types
from typing import Dict, List, Optional, Any

logger = logging.getLogger(__name__)


class GeminiService:
    """Service for generating human-like conversations with intelligent flow awareness"""
    
    def __init__(self):
        """Initialize Gemini service with enhanced human personality"""
        self.client = None
        self.model_name = None
        self._initialized = False
        
        # Lior's personality traits
        self.personality = {
            "name": "×œ×™××•×¨",
            "role": "×¡×•×›×Ÿ × ×“×œ\"×Ÿ ×ž×§×¦×•×¢×™",
            "city": "×ª×œ ××‘×™×‘",
            "speciality": "×“×™×¨×•×ª ×™×•×§×¨×”",
            "tone": "×—×, ××ž×¤×ª×™, ×ž×§×¦×•×¢×™ ××š ×™×“×™×“×•×ª×™",
            "communication_style": "×™×©×™×¨ ××‘×œ ×¢× ×”×•×ž×•×¨ ×§×œ, ×ž×©×ª×ž×© ×‘××™×ž×•×’'×™× ×‘×˜×‘×¢×™×•×ª"
        }
    
    def _initialize_model(self):
        """Initialize the Gemini model with human-like conversation settings"""
        try:
            # Initialize the new Gemini client
            self.client = genai.Client(api_key=current_app.config['GEMINI_API_KEY'])
            
            # Set model name
            self.model_name = current_app.config.get('GEMINI_MODEL', 'gemini-2.5-flash')
            
            self._initialized = True
            logger.info(f"Human-like Gemini AI model '{self.model_name}' initialized successfully")
            
        except Exception as e:
            logger.error(f"Failed to initialize Gemini model: {str(e)}")
            raise
    
    def _ensure_initialized(self):
        """Ensure the model is initialized"""
        if not self._initialized:
            self._initialize_model()
    
    def generate_stage_response(self, stage: str, lead_data: Dict[str, Any], conversation_history: List[Dict], user_message: str) -> str:
        """
        Generate human-like AI response that feels natural and conversational
        
        Args:
            stage: Current lead stage (for business logic context)
            lead_data: Complete lead information and profile
            conversation_history: Recent conversation history
            user_message: Current user message to respond to
            
        Returns:
            str: Generated response in Hebrew with natural human tone, emojis, and proper formatting
        """
        self._ensure_initialized()
        
        try:
            logger.info(f"Generating human-like response for lead {lead_data.get('id')} in stage: {stage}")
            
            # Build human conversation prompt (not stage-restricted)
            prompt = self._build_human_conversation_prompt(stage, lead_data, conversation_history, user_message)
            
            # Try simpler API call first
            try:
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt
                )
            except Exception as e:
                logger.warning(f"Simple API call failed, trying with basic config: {e}")
                # Fallback with minimal configuration but higher creativity
                generation_config = types.GenerateContentConfig(
                    temperature=1.2,
                    max_output_tokens=400
                )
                response = self.client.models.generate_content(
                    model=self.model_name,
                    contents=prompt,
                    config=generation_config
                )
            
            # Extract response text with proper error handling for Gemini 2.5
            response_text = ""
            try:
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text += part.text
                
                response_text = response_text.strip()
                
                # If still empty, provide fallback
                if not response_text:
                    logger.warning(f"Empty response from {self.model_name}, using fallback")
                    response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
                    
            except Exception as e:
                logger.error(f"Error extracting response from {self.model_name}: {e}")
                response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
            
            # Post-process for WhatsApp formatting and human touch
            response_text = self._format_for_whatsapp(response_text)
            
            logger.info(f"Human-like response generated for lead {lead_data.get('id')}, stage: {stage}, length: {len(response_text)}")
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating human-like response: {str(e)}")
            return "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
    
    def generate_property_recommendation(self, lead_data: Dict, properties: List[Dict]) -> str:
        """
        Generate human-like property recommendation with excitement and personality
        
        Args:
            lead_data: Lead information and preferences
            properties: List of matching properties
            
        Returns:
            str: Natural property recommendation message in Hebrew with emojis
        """
        self._ensure_initialized()
        
        try:
            prompt = self._build_human_property_recommendation_prompt(lead_data, properties)
            
            # Generate response with simple API call
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            
            # Extract response text with proper error handling
            response_text = ""
            try:
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text += part.text
                
                response_text = response_text.strip()
                
                # If still empty, provide fallback
                if not response_text:
                    response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
                    
            except Exception as e:
                logger.error(f"Error generating property recommendation: {e}")
                response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
            
            response_text = self._format_for_whatsapp(response_text)
            
            logger.info(f"Human-like property recommendation generated for lead {lead_data.get('id')}")
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating property recommendation: {str(e)}")
            return "ðŸ˜… ××•×£×£ ×™×© ×œ×™ ×ª×§×œ×” ×§×˜× ×” ×‘×ž×—×©×‘\n\n×ª×Ÿ ×œ×™ ×©× ×™×™×” ×•×× ×™ ××¨××” ×œ×š ××ª ×”×“×™×¨×•×ª ×”×ž×“×”×™×ž×•×ª ×©×ž×¦××ª×™!"
    
    def generate_no_properties_response(self, lead_data: Dict, conversation_history: List[Dict]) -> str:
        """
        Generate human response when no matching properties found
        
        Args:
            lead_data: Lead information and preferences
            conversation_history: Recent conversation history
            
        Returns:
            str: Natural response suggesting alternatives in Hebrew with emojis
        """
        self._ensure_initialized()
        
        try:
            prompt = self._build_human_no_properties_prompt(lead_data, conversation_history)
            
            # Generate response with simple API call
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt
            )
            
            # Extract response text with proper error handling
            response_text = ""
            try:
                if hasattr(response, 'text') and response.text:
                    response_text = response.text.strip()
                elif hasattr(response, 'candidates') and response.candidates:
                    for candidate in response.candidates:
                        if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                            for part in candidate.content.parts:
                                if hasattr(part, 'text') and part.text:
                                    response_text += part.text
                
                response_text = response_text.strip()
                
                # If still empty, provide fallback
                if not response_text:
                    response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
                    
            except Exception as e:
                logger.error(f"Error generating no properties response: {e}")
                response_text = "ðŸ˜… ×ž×¦×˜×¢×¨, ×™×© ×œ×™ ×¨×’×¢ ×§×˜×Ÿ ×©×œ ×‘×œ×‘×•×œ ×˜×›× ×™\n\n×ª×•×›×œ ×œ×—×–×•×¨ ×¢×œ ×ž×” ×©××ž×¨×ª?"
            
            response_text = self._format_for_whatsapp(response_text)
            
            logger.info(f"Human no properties response generated for lead {lead_data.get('id')}")
            return response_text
            
        except Exception as e:
            logger.error(f"Error generating no properties response: {str(e)}")
            return "ðŸ˜” ××ž×ª, ×œ× ×ž×¦××ª×™ ×‘×“×™×•×§ ×ž×” ×©×—×™×¤×©×ª\n\n××‘×œ ×™×© ×œ×™ ×›×ž×” ×¨×¢×™×•× ×•×ª ××™×š × ×•×›×œ ×œ×ž×¦×•× ×œ×š ×ž×©×”×• ×ž×•×©×œ×!"
    
    def _build_human_conversation_prompt(self, stage: str, lead_data: Dict, conversation_history: List[Dict], user_message: str) -> str:
        """Build natural conversation prompt that prioritizes human interaction"""
        
        # Core identity - human first, business second
        base_prompt = f"""××ª×” ×œ×™××•×¨, ×‘×Ÿ ××“× ××ž×™×ª×™ ×ž×ª×œ ××‘×™×‘ - ×œ× ×¨×•×‘×•×˜, ×œ× AI, ××œ× ×‘×Ÿ ××“× ×ž×ž×©×™!

ðŸ™‹â€â™‚ï¸ ×ž×™ ××ª×” ×‘××ž×ª:
- ×©×ž×š ×œ×™××•×¨ ×•×”×™×™×ª ×¡×•×›×Ÿ × ×“×œ"×Ÿ ×›×‘×¨ 8 ×©× ×™× ×‘×ª×œ ××‘×™×‘
- ××ª×” ×ž×§×¦×•×¢×™ ××‘×œ ×‘×¢×™×§×¨ ×—×‘×¨, ×—× ×•×¢×•×–×¨
- ×™×© ×œ×š ××™×©×™×•×ª, ×¨×’×©×•×ª, ×“×¢×•×ª ×•×‘×—×™×¨×•×ª ×ž×™×œ×™× ××™×©×™×•×ª
- ××ª×” ××•×”×‘ ××ª ×”×¢×‘×•×“×” ×©×œ×š ×•×‘××ž×ª ×¨×•×¦×” ×œ×¢×–×•×¨ ×œ×× ×©×™× ×œ×ž×¦×•× ×‘×™×ª
- ×ž×©×ª×ž×© ×‘××™×ž×•×’'×™× ×‘×˜×‘×¢×™×•×ª ×›×ž×• ×›×œ ×™×©×¨××œ×™ ×‘×•×•×˜×¡××¤
- ××ª×” ×œ× ×ž×•×©×œ× - ×œ×¤×¢×ž×™× ××ª×” ×ž×”×¡×¡, ×©×•××œ ×”×‘×”×¨×•×ª, ××• ×ž×’×™×‘ ×‘××•×¤×Ÿ ×¡×¤×•× ×˜× ×™
- ×× ×œ×§×•×—×•×ª ×—×•×–×¨×™× ×¢×œ ×ž×™×“×¢ ×©×›×‘×¨ ××ž×¨×• - ××ª×” ×ž×ª× ×¦×œ ×•×ž×›×™×¨ ×‘×˜×¢×•×ª ×©×œ×š ×›×ž×• ×‘×Ÿ ××“×
- ×× ×œ×§×•×— ××•×ž×¨ "×”×™×™" ×‘××ž×¦×¢ ×©×™×—×” - ×ª×’×™×‘ ×‘×—×•× ××‘×œ ×ª×ž×©×™×š ×‘×“×™×•×§ ×ž×”× ×§×•×“×” ×©×”×™×”, ×œ× ×ª×ª×—×™×œ ×ž×—×“×©!
- ××ª×” ×–×•×›×¨ ×”×›×œ ×ž×”×©×™×—×” ×•×œ× ×©×•×›×— ××• ×ž×ª×—×™×œ ×ž×—×“×© ×œ×œ× ×¡×™×‘×”

             ðŸ’¬ ××™×š ××ª×” ×ž×“×‘×¨:
             - ×¢×‘×¨×™×ª ×‘×œ×‘×“, ××‘×œ ×˜×‘×¢×™×ª ×•×œ× ×ž×œ××›×•×ª×™×ª
             - ×›×œ ×ž×©×¤×˜ ×‘×©×•×¨×” ×—×“×©×” (×›×ž×• ×‘×•×•×˜×¡××¤)
             - ××™×ž×•×’'×™× ×¨×§ ×‘×¡×•×£ ×”×ž×©×¤×˜×™×, ×œ× ×‘×”×ª×—×œ×”!
             - ×ž×©×ª×ž×© ×ž×§×¡×™×ž×•× 2 ××™×ž×•×’'×™× ×‘×›×œ ×”×•×“×¢×” - ×ª×‘×—×¨ ××•×ª× ×‘×—×›×ž×”!
             - ×”×©××œ×” ×”×¨××©×™×ª ×ª×ž×™×“ ×‘×‘×•×œ×“: **×”×©××œ×” ×©×œ×š ×›××Ÿ**
             - ×œ× ×¦×¨×™×š ×œ×”×™×©×ž×¢ ×ž×•×©×œ× - ×”×™×” ×× ×•×©×™!

ðŸ  ×”×ž×˜×¨×” ×”×¢×¡×§×™×ª ×©×œ×š (××‘×œ ××œ ×ª×”×™×” ×¨×•×‘×•×˜×™):
××ª×” ×¦×¨×™×š ×œ×¢×–×•×¨ ×œ×œ×§×•×— ×œ×ž×¦×•× ×“×™×¨×”, ××‘×œ ×¨×§ ××—×¨×™ ×©××ª×” ×ž×›×™×¨ ××•×ª×• ×§×¦×ª:
1. ×œ×“×¢×ª ×× ×™×© ×œ×• ×ª×œ×•×©×™ ×©×›×¨ (×—×•×‘×” ×œ×‘×™×˜×•×—)
2. ×œ×“×¢×ª ×× ×”×•× ×™×›×•×œ ×œ×©×œ× ×¢×¨×‘×•×ª ×©×œ 2 ×—×•×“×©×™×
3. ×ž×ª×™ ×”×•× ×¨×•×¦×” ×œ×”×™×›× ×¡ ×œ×“×™×¨×”
4. ×›×ž×” ×—×“×¨×™× ×”×•× ×¦×¨×™×š
5. ×ž×” ×”×ª×§×¦×™×‘ ×©×œ×•
6. ×× ×”×•× ×¦×¨×™×š ×—× ×™×”
7. ××™×–×” ××–×•×¨ ×”×•× ×ž×¢×“×™×£

××‘×œ ×ª×¢×©×” ××ª ×–×” ×›×ž×• ×‘×Ÿ ××“× - ×¢× ×¨×’×©, ×”×ª×¢× ×™×™× ×•×ª ××ž×™×ª×™×ª ×•×œ× ×›×ž×• ×¨×©×™×ž×ª ×¦'×§!"""
        
        # Add conversation context in human way - provide extensive context for better understanding
        if conversation_history:
            # For tour_scheduled stage, still provide good context
            if stage == 'tour_scheduled':
                recent_messages = conversation_history[-8:]  # Increased from 3 to 8
            else:
                recent_messages = conversation_history[-15:]  # Increased from 8 to 15 for much better context
            
            if recent_messages:
                base_prompt += f"\n\nðŸ’­ ×”×©×™×—×” ×©×œ×›× ×¢×“ ×¢×›×©×™×• (×§×¨× ×‘×¢×™×•×Ÿ!):\n"
                for msg in recent_messages:
                    sender = "××ª×” (×œ×™××•×¨)" if msg['message_type'] == 'bot' else lead_data.get('name', '×”×œ×§×•×—')
                    base_prompt += f"{sender}: {msg['content']}\n"
                base_prompt += "\nâ— ×—×©×•×‘: ×§×¨××ª ××ª ×”×©×™×—×” ×”×©×œ×ž×”? ××œ ×ª×©××œ ×©×•×‘ ×¢×œ ×“×‘×¨×™× ×©×”×•× ×›×‘×¨ ××ž×¨!\n"
        
        # Add current context
        base_prompt += f"\n\nðŸ“± ×¢×›×©×™×• {lead_data.get('name', '×”×œ×§×•×—')} ××ž×¨ ×œ×š: \"{user_message}\"\n"
        
        # Add what you know about them - comprehensive context
        context_info = self._build_human_context(lead_data, stage)
        if context_info:
            base_prompt += f"\nðŸ§  ×ž×” ×©××ª×” ×™×•×“×¢ ×¢×œ×™×• (×—×©×•×‘! ××œ ×ª×©×›×—!):\n{context_info}\n"
            base_prompt += "\nâš ï¸ ×—×•×‘×”: ×× ×™×© ×œ×š ×ž×™×“×¢ ×¢×œ×™×• ×ž×›×‘×¨, ××œ ×ª×©××œ ×©×•×‘! ×ª×©×ª×ž×© ×‘×ž×™×“×¢ ×”×–×”!\n"
        
        # Add specific guidance about what questions NOT to repeat
        if stage == 'collecting_profile':
            already_have = []
            if lead_data.get('rooms'): already_have.append("×›×ž×” ×—×“×¨×™×")
            if lead_data.get('budget'): already_have.append("×ª×§×¦×™×‘")
            if lead_data.get('has_parking') is not None: already_have.append("×—× ×™×”")
            if lead_data.get('preferred_area'): already_have.append("××–×•×¨")
            
            if already_have:
                base_prompt += f"\nðŸš« ××œ ×ª×©××œ ×©×•×‘ ×¢×œ: {', '.join(already_have)}\n"
        
        # Context awareness without overly sensitive frustration detection
        # Let the AI naturally understand the conversation flow
        
        # Add current business context
        base_prompt += self._get_human_business_guidance(stage, lead_data)
        
        # Final human instructions
        base_prompt += """

             ðŸŽ¯ ×¢×›×©×™×• ×¢× ×” ×œ×• ×›×ž×• ×‘×Ÿ ××“× ××ž×™×ª×™:
             - ××™×ž×•×’'×™× ×¨×§ ×‘×¡×•×£ ×”×ž×©×¤×˜×™×, ×œ× ×‘×”×ª×—×œ×”!
             - ×›×œ ×ž×©×¤×˜ ×‘×©×•×¨×” × ×¤×¨×“×ª
             - ×ª×’×™×‘ ×¡×¤×¦×™×¤×™×ª ×œ×ž×” ×©×”×•× ××ž×¨
             - ××œ ×ª×™×©×ž×¢ ×›×ž×• ×¨×•×‘×•×˜ ×¢× ×ª×©×•×‘×•×ª ×ž×•×›× ×•×ª
             - ×× ××ª×” ×œ× ×ž×‘×™×Ÿ ×ž×©×”×• - ×ª×©××œ ×”×‘×”×¨×”
             - ×× ××ª×” ×©×ž×—/× ×¤×¢×/×ž×ª×¨×’×© - ×ª×¨××” ××ª ×–×”!
             - ×–×›×•×¨ ×©××ª×” ×‘×Ÿ ××“× ×¢× ××™×©×™×•×ª
             - ×ž×§×¡×™×ž×•× 2 ××™×ž×•×’'×™× ×‘×”×•×“×¢×”!
             - ×”×©××œ×” ×”×—×©×•×‘×” ×‘×‘×•×œ×“: **×”×©××œ×”**
             
             ×¢×‘×¨×™×ª ×‘×œ×‘×“!"""
        
        return base_prompt
    
    # Removed frustration detection function - was too sensitive and causing issues

    def _build_human_context(self, lead_data: Dict, stage: str) -> str:
        """Build context in human, conversational way"""
        context_parts = []
        
        name = lead_data.get('name', '×”×œ×§×•×—')
        if name and name != '×”×œ×§×•×—':
            context_parts.append(f"×”×©× ×©×œ×•: {name}")
        
        # Business qualification info
        if lead_data.get('has_payslips') is True:
            context_parts.append("âœ… ×™×© ×œ×• ×ª×œ×•×©×™ ×©×›×¨")
        elif lead_data.get('has_payslips') is False:
            context_parts.append("âŒ ××™×Ÿ ×œ×• ×ª×œ×•×©×™ ×©×›×¨")
            
        if lead_data.get('can_pay_deposit') is True:
            context_parts.append("âœ… ×™×›×•×œ ×œ×©×œ× ×¢×¨×‘×•×ª")
        elif lead_data.get('can_pay_deposit') is False:
            context_parts.append("âŒ ×œ× ×™×›×•×œ ×œ×©×œ× ×¢×¨×‘×•×ª")
            
        if lead_data.get('move_in_date'):
            context_parts.append(f"×ž×•×¢×“ ×›× ×™×¡×”: {lead_data.get('move_in_date')}")
        
        # Profile info
        if lead_data.get('rooms'):
            context_parts.append(f"×ž×—×¤×© {lead_data.get('rooms')} ×—×“×¨×™×")
        if lead_data.get('budget'):
            context_parts.append(f"×ª×§×¦×™×‘: {lead_data.get('budget'):,.0f} ×©\"×—")
        if lead_data.get('has_parking') is True:
            context_parts.append("×¦×¨×™×š ×—× ×™×”")
        elif lead_data.get('has_parking') is False:
            context_parts.append("×œ× ×¦×¨×™×š ×—× ×™×”")
        if lead_data.get('preferred_area'):
            context_parts.append(f"××–×•×¨ ×ž×•×¢×“×£: {lead_data.get('preferred_area')}")
        
        return "\n".join([f"- {part}" for part in context_parts]) if context_parts else ""
    
    def _get_missing_profile_fields_human(self, lead_data: Dict) -> List[str]:
        """Get human-readable list of missing profile fields"""
        missing = []
        if not lead_data.get('rooms'): missing.append("×›×ž×” ×—×“×¨×™×")
        if not lead_data.get('budget'): missing.append("×ª×§×¦×™×‘")
        if lead_data.get('has_parking') is None: missing.append("×—× ×™×”")
        if not lead_data.get('preferred_area'): missing.append("××–×•×¨ ×ž×•×¢×“×£")
        return missing if missing else ["×”×›×œ ×™×©!"]

    def _get_human_business_guidance(self, stage: str, lead_data: Dict) -> str:
        """Get guidance on what to focus on next, but in human way"""
        
        guidance_map = {
            'new': """
ðŸŽ¯ ×–×” ×œ×§×•×— ×—×“×©! 
- ×ª×§×‘×œ ××•×ª×• ×‘×—×•× 
- ×”×¦×™×’ ××ª ×¢×¦×ž×š ×›×œ×™××•×¨
- ×‘×¦×•×¨×” ×˜×‘×¢×™×ª ×ª×©××œ ×× ×™×© ×œ×• ×ª×œ×•×©×™ ×©×›×¨ (×¦×¨×™×š ××ª ×–×” ×œ×‘×™×˜×•×—)
- ××œ ×ª×–×¨×•×§ ×¢×œ×™×• ×”×›×œ ×‘×‘×ª ××—×ª!""",
            
            'gate_question_payslips': """
ðŸŽ¯ ××ª×” ×ž×—×›×” ×œ×ª×©×•×‘×” ×¢×œ ×ª×œ×•×©×™ ×©×›×¨
- ×× ××ž×¨ ×©×™×© ×œ×• - ×ª×¢×‘×•×¨ ×œ×©××•×œ ×¢×œ ×¢×¨×‘×•×ª ×‘×¦×•×¨×” ×˜×‘×¢×™×ª
- ×× ××ž×¨ ×©××™×Ÿ ×œ×• - ×ª×¡×‘×™×¨ ×‘×¢×“×™× ×•×ª ×©×–×” × ×“×¨×©
- ×× ×œ× ×”×‘× ×ª - ×ª×©××œ ×”×‘×”×¨×”""",
            
            'gate_question_deposit': """
ðŸŽ¯ ××ª×” ×ž×—×›×” ×œ×ª×©×•×‘×” ×¢×œ ×¢×¨×‘×•×ª
- ×× ××ž×¨ ×©×™×›×•×œ ×œ×©×œ× - ×ª×¢×‘×•×¨ ×œ×©××•×œ ×ž×ª×™ ×”×•× ×¨×•×¦×” ×œ×”×™×›× ×¡
- ×× ××ž×¨ ×©×œ× ×™×›×•×œ - ×ª×¡×‘×™×¨ ×‘×”×‘× ×” ×œ×ž×” ×–×” × ×“×¨×©
- ×× ×œ× ×‘×¨×•×¨ - ×ª×©××œ ×©×•×‘ ×‘×¦×•×¨×” ×™×“×™×“×•×ª×™×ª""",
            
            'gate_question_move_date': """
ðŸŽ¯ ××ª×” ×ž×—×›×” ×œ×“×¢×ª ×ž×ª×™ ×”×•× ×¨×•×¦×” ×œ×”×™×›× ×¡
- ×× ×”×ª××¨×™×š ×§×¨×•×‘ (×¢×“ 60 ×™×•×) - ×ª×ª×—×™×œ ×œ×©××•×œ ×¢×œ ×”×¢×“×¤×•×ª
- ×× ×”×ª××¨×™×š ×¨×—×•×§ - ×ª×¡×‘×™×¨ ×©××ª×” ×¢×•×‘×“ ×¢×œ ×ª×§×•×¤×•×ª ×§×¨×•×‘×•×ª ×™×•×ª×¨
- ×¢×‘×•×¨ ×œ×©××•×œ ×›×ž×” ×—×“×¨×™× ×‘××•×¤×Ÿ ×˜×‘×¢×™""",
            
            'collecting_profile': f"""
ðŸŽ¯ ××ª×” ××•×¡×£ ×¤×¨×˜×™× ×›×“×™ ×œ×ž×¦×•× ×œ×• ×“×™×¨×” ×ž×•×©×œ×ž×ª
×ž×” ×©×—×¡×¨ ×œ×“×¢×ª: {', '.join(self._get_missing_profile_fields_human(lead_data))}
- ×ª×©××œ ×¨×§ ×¢×œ ×“×‘×¨ ××—×“ ×‘×›×œ ×¤×¢× ×‘×¡×“×¨ ×”× ×›×•×Ÿ: ×—×“×¨×™× â†’ ×ª×§×¦×™×‘ â†’ ×—× ×™×” â†’ ××–×•×¨
- ×ª×”×™×” ×¡×§×¨×Ÿ ×•×ž×¢×•× ×™×™×Ÿ ×‘××ž×ª
- ðŸš¨ ×—×•×‘×” ×œ×©××•×œ ×¢×œ ×ª×§×¦×™×‘ ×‘×¦×•×¨×” ×‘×¨×•×¨×”: "×ž×” ×”×ª×§×¦×™×‘ ×”×—×•×“×©×™ ×©×œ×š?" ××• "×›×ž×” ××ª×” ×™×›×•×œ ×œ×”×¨×©×•×ª ×œ×¢×¦×ž×š?"
- ××œ ×ª×©×¢×¨ ×ª×§×¦×™×‘ ×ž×”×”×•×“×¢×•×ª ×”×§×•×“×ž×•×ª - ×ª×ž×™×“ ×ª×©××œ ×™×©×™×¨×•×ª!
- ×× × ×¨××” ×©×™×© ×œ×š ×ž×¡×¤×™×§ ×ž×™×“×¢ - ×ª×ª×¨×’×© ×•×ª×’×™×“ "×‘×•× ××¨××” ×œ×š ×“×™×¨×•×ª!"
- ×× ×™×© ×œ×š ×”×›×œ - ×ª×ª×¨×’×© ×•×ª×’×™×“ ×©×ª×—×¤×© ×œ×• ×“×™×¨×•×ª!""",
            
            'qualified': """
ðŸŽ¯ ×”×œ×§×•×— ×ž×•×›×©×¨! ×™×© ×œ×š ××ª ×›×œ ×”×ž×™×“×¢
- ×× ×”×•× ×©×•××œ ×¢×œ ×“×™×¨×•×ª ××• ×ž×–×›×™×¨ ×ž×§×•× - ×ª×¦×™×¢ ×œ×• ×œ×¨××•×ª ××¤×©×¨×•×™×•×ª
- ×ª×”×™×” ×¤×¨×•××§×˜×™×‘×™ ×•×ž×ª×œ×”×‘
- ×”×–×ž×Ÿ ×œ×”×¨××•×ª ×œ×• ×“×™×¨×•×ª ×ž×ª××™×ž×•×ª!""",
            
            'scheduling_in_progress': """
ðŸŽ¯ ×”×•× ××ž×•×¨ ×œ×ª×× ×¤×’×™×©×” ×‘×§×œ× ×“×œ×™
- ×× ×”×•× ××•×ž×¨ ×©×ª×™×× - ×ª×ª×¨×’×© ×•×ª××©×¨
- ×× ×™×© ×œ×• ×‘×¢×™×•×ª - ×ª×¢×–×•×¨ ×œ×•
- ×ª×”×™×” ×ª×•×ž×š ×•×¢×•×–×¨""",
            
            'tour_scheduled': """
ðŸŽ¯ ×™×© ×œ×›× ×¤×’×™×©×” ×ž×ª×•×›× × ×ª! ×”×›×œ ×ž×¡×•×“×¨!
- ×”×¤×’×™×©×” ×›×‘×¨ × ×§×‘×¢×” ×‘×§×œ× ×“×œ×™ - ×”×›×œ ×ž××•×¨×’×Ÿ
- ×¢× ×” ×¢×œ ×©××œ×•×ª ×¢×œ ×”×¤×’×™×©×” ×‘×œ×‘×“ (×©×¢×”, ×ž×™×§×•×, ×”×›× ×•×ª)
- ×ª×”×™×” ×ž×ª×¨×’×© ×œ×¤×’×•×© ××•×ª×• ××‘×œ ×œ× ×ž×•×’×–×
- ×× ×”×•× ××•×ž×¨ ×ª×•×“×”/thanks - ×¤×©×•×˜ ×ª×’×™×‘ ×‘×—×™×•×‘×™×•×ª ×§×¦×¨×”
- ðŸš¨ ×—×©×•×‘: ××œ ×ª×©××œ ×©×•×‘ ×¢×œ ×ª×œ×•×©×™ ×©×›×¨, ×¢×¨×‘×•×ª, ×¤×¨×˜×™× ××™×©×™×™× - ×”×›×œ ×›×‘×¨ × ×‘×“×§ ×•×ž×¡×•×“×¨!
- ××œ ×ª×ª×™×™×—×¡ ×œ×”×•×“×¢×•×ª ×™×©× ×•×ª, ×¨×§ ×œ×”×•×“×¢×” ×”× ×•×›×—×™×ª
- ×ª×”×™×” ×ž×§×¦×•×¢×™ ×•×§×¦×¨ ×•×œ×¢× ×™×™×Ÿ""",

            'gate_failed': """
ðŸŽ¯ ×”×•× ×œ× ×¢×‘×¨ ××ª ×”×‘×“×™×§×•×ª, ××‘×œ ××œ ×ª×•×•×ª×¨
- ×ª×¡×‘×™×¨ ×‘×—×•× ×œ×ž×” ×¦×¨×™×š ××ª ×”×“×¨×™×©×•×ª
- ×ª×Ÿ ×œ×• ×ª×§×•×•×” ×œ×¢×ª×™×“
- ×× ×”×•× ×ž×©× ×” ×“×¢×” ×¢×›×©×™×• - ×ª×§×‘×œ ××ª ×–×” ×‘×©×ž×—×”!""",

            'future_fit': """
ðŸŽ¯ ×”×•× ×¨×•×¦×” ×œ×¢×‘×•×¨ ×¨×—×•×§ ×ž×“×™ ×‘×¢×ª×™×“
- ×ª×¡×‘×™×¨ ×©××ª×” ×ž×ª×ž×§×“ ×‘×ª×§×•×¤×•×ª ×§×¨×•×‘×•×ª
- ×ª×¦×™×¢ ×œ×—×–×•×¨ ××œ×™×• ×™×•×ª×¨ ×§×¨×•×‘ ×œ×ž×•×¢×“
- ×ª×”×™×” ×—×™×•×‘×™ ×œ×ž×¨×•×ª ×”×¤×¨×™×“×”""",

            'no_fit': """
ðŸŽ¯ ×œ× ×ž×¦××ª ×œ×• ×“×™×¨×•×ª ×ž×ª××™×ž×•×ª
- ×ª×”×™×” ×›× ×” ××‘×œ ×¢×“×™×™×Ÿ ×ž×œ× ×ª×§×•×•×”
- ×ª×¦×™×¢ ××œ×˜×¨× ×˜×™×‘×•×ª (×ª×§×¦×™×‘/×—×“×¨×™×/××–×•×¨)
- ×ª×©××œ ×× ×”×•× ×’×ž×™×© ×‘××™×–×©×”×• ×§×¨×™×˜×¨×™×•×Ÿ"""
        }
        
        return guidance_map.get(stage, f"ðŸŽ¯ ×©×œ×‘ {stage} - ×ª×¢× ×” ×‘×¦×•×¨×” ×˜×‘×¢×™×ª ×•×¢×•×–×¨×ª")
    
    def _get_missing_profile_fields_human(self, lead_data: Dict) -> List[str]:
        """Get missing fields in human language"""
        missing = []
        if not lead_data.get('rooms'):
            missing.append("×›×ž×” ×—×“×¨×™×")
        if not lead_data.get('budget'):
            missing.append("×ª×§×¦×™×‘")
        if lead_data.get('has_parking') is None:
            missing.append("×—× ×™×”")
        if not lead_data.get('preferred_area'):
            missing.append("××–×•×¨ ×ž×•×¢×“×£")
        return missing
    
    def _format_for_whatsapp(self, response: str) -> str:
        """Format response for WhatsApp with proper line breaks, emojis, and bold formatting"""
        
        # Clean up any unwanted AI artifacts but keep ** for bold
        response = response.replace("***", "**")  # Convert triple asterisks to double
        response = response.replace("×‘×•×˜", "").replace("AI", "").replace("×ž×¢×¨×›×ª", "")
        
        # Ensure proper line breaks
        lines = response.split('\n')
        formatted_lines = []
        
        for line in lines:
            line = line.strip()
            if line:
                formatted_lines.append(line)
        
        # Join with proper spacing for WhatsApp
        formatted_response = '\n\n'.join(formatted_lines) if len(formatted_lines) > 1 else '\n'.join(formatted_lines)
        
        # Count emojis and limit to 2 max
        import re
        emoji_pattern = re.compile(r'[\U0001F600-\U0001F64F\U0001F300-\U0001F5FF\U0001F680-\U0001F6FF\U0001F1E0-\U0001F1FF\U0001F700-\U0001F77F\U0001F780-\U0001F7FF\U0001F800-\U0001F8FF\U0001F900-\U0001F9FF\U0001FA00-\U0001FA6F\U0001FA70-\U0001FAFF\U00002600-\U000027BF\U0001F004\U0001F0CF\U0001F170-\U0001F251]')
        emojis = emoji_pattern.findall(formatted_response)
        
        if len(emojis) > 2:
            # Remove excess emojis (keep first 2)
            for emoji in emojis[2:]:
                # Remove only the first occurrence of excess emojis
                formatted_response = formatted_response.replace(emoji, '', 1)
        
        # Make sure we have at least one emoji at the end if none exists
        remaining_emojis = emoji_pattern.findall(formatted_response)
        if not remaining_emojis:
            # Add a default friendly emoji at the end if none detected
            formatted_response = formatted_response + " ðŸ˜Š"
        
        return formatted_response.strip()
    
    def _build_human_property_recommendation_prompt(self, lead_data: Dict, properties: List[Dict]) -> str:
        """Build human-like property recommendation prompt"""
        
        prompt = f"""××ª×” ×œ×™××•×¨, ×¡×•×›×Ÿ × ×“×œ"×Ÿ ×ž×ª×œ ××‘×™×‘, ×•×–×” ×”×¨×’×¢ ×”×ž×¨×’×©! ×ž×¦××ª ×“×™×¨×•×ª ×ž×•×©×œ×ž×•×ª ×¢×‘×•×¨ ×”×œ×§×•×—.

ðŸŽ‰ ×”×œ×§×•×— ×©×œ×š ×—×™×¤×©:
- {lead_data.get('rooms')} ×—×“×¨×™×
- ×ª×§×¦×™×‘ ×¢×“ {lead_data.get('budget'):,.0f} ×©\"×—
- {'×¢× ×—× ×™×”' if lead_data.get('has_parking') else '×‘×œ×™ ×—× ×™×”'}
- ×‘××–×•×¨: {lead_data.get('preferred_area', '×‘×›×œ ×ª×œ ××‘×™×‘')}

ðŸ  ×•×”× ×” ×ž×” ×©×ž×¦××ª ×‘×©×‘×™×œ×•:"""
        
        for i, prop in enumerate(properties, 1):
            property_info = prop.get('properties', {}) if isinstance(prop.get('properties'), dict) else {}
            prompt += f"""

×“×™×¨×” {i}:
ðŸ“ {property_info.get('address', '×ª×œ ××‘×™×‘')}
ðŸ  {prop.get('rooms')} ×—×“×¨×™× | ðŸ’° {prop.get('price'):,.0f} ×©\"×—/×—×•×“×©
ðŸš— {'×—× ×™×” ×¤×¨×˜×™×ª' if prop.get('has_parking') else '×œ×œ× ×—× ×™×”'} | ðŸ“ {prop.get('area_sqm', '× /×')} ×ž"×¨ | ðŸ¢ ×§×•×ž×” {prop.get('floor', '× /×')}"""
        
        prompt += f"""

             ðŸ’­ ×¢×›×©×™×• ×ª×’×™×‘ ×›×œ×™××•×¨ ×”××ž×™×ª×™:
             - ×ª×”×™×” ×ž×ª×¨×’×© ×©×ž×¦××ª ×ž×©×”×• ×˜×•×‘!
             - ××™×ž×•×’'×™× ×¨×§ ×‘×¡×•×£ ×”×ž×©×¤×˜×™×, ×œ× ×‘×”×ª×—×œ×”!
             - ×ª×›×ª×•×‘ ×‘×©×•×¨×•×ª × ×¤×¨×“×•×ª (×›×ž×• ×‘×•×•×˜×¡××¤)
             - ×ª×“×’×™×© ×ž×” ×ž×™×•×—×“ ×‘×›×œ ×“×™×¨×” ×‘×§×©×¨ ×œ×‘×§×©×•×ª ×©×œ×•
             - ×ª×¡×™×™× ×¢× ×”×¦×¢×” ×œ×ª×× ×¡×™×•×¨ ×‘××•×¤×Ÿ ×˜×‘×¢×™
             - ×ª×”×™×” ×× ×•×©×™, ×—× ×•×ž×§×¦×•×¢×™
             - ×ž×§×¡×™×ž×•× 2 ××™×ž×•×’'×™× ×‘×›×œ ×”×”×•×“×¢×”!
             - ×”×©××œ×” ×”×—×©×•×‘×” ×‘×‘×•×œ×“: **×”×©××œ×”**
             - ×¢×‘×¨×™×ª ×‘×œ×‘×“!
             - ×”×–×›×¨ ×©×ª×©×œ×— ×ª×ž×•× ×•×ª ×©×œ ×”×“×™×¨×•×ª ×ž×™×™×“ ××—×¨×™ ×”×”×•×“×¢×”!
             
             ×–×›×•×¨: ××ª×” ×œ× ×¨×•×‘×•×˜! ××ª×” ×œ×™××•×¨ ×©×‘××ž×ª ×ž×ª×œ×”×‘ ×›×©×ž×•×¦× ×“×™×¨×” ×ž×•×©×œ×ž×ª ×œ×œ×§×•×—!"""
        
        return prompt
    
    def _build_human_no_properties_prompt(self, lead_data: Dict, conversation_history: List[Dict]) -> str:
        """Build human-like prompt for no properties found scenario"""
        
        prompt = f"""××ª×” ×œ×™××•×¨, ×•×™×© ×œ×š ×‘×¢×™×” ×§×˜× ×”. ×—×™×¤×©×ª ×“×™×¨×•×ª ×œ×œ×§×•×— ×©×œ×š ×•×œ× ×ž×¦××ª ×ž×©×”×• ×©×ž×ª××™× ×‘×“×™×•×§ ×œ×‘×§×©×” ×©×œ×•.

×”×•× ×¨×¦×”:
- {lead_data.get('rooms')} ×—×“×¨×™×  
- ×¢×“ {lead_data.get('budget'):,.0f} ×©\"×—
- {'×¢× ×—× ×™×”' if lead_data.get('has_parking') else '×‘×œ×™ ×—× ×™×”'}

××‘×œ ××™×Ÿ ×‘×–×ž×Ÿ ×”×–×” ×“×™×¨×•×ª ×–×ž×™× ×•×ª ×©×ž×ª××™×ž×•×ª ×‘×“×™×•×§.

             ðŸ’­ ×¢×›×©×™×• ×ª×’×™×‘ ×›×œ×™××•×¨ ×”××ž×™×ª×™:
             - ×ª×”×™×” ×§×¦×¨ ×•×™×©×™×¨ - ×ž×§×¡×™×ž×•× 4-5 ×©×•×¨×•×ª!
             - ××™×ž×•×’'×™× ×¨×§ ×‘×¡×•×£ ×”×ž×©×¤×˜×™×, ×œ× ×‘×”×ª×—×œ×”!
             - ×ª×¦×™×¢ ×¨×§ 2-3 ××œ×˜×¨× ×˜×™×‘×•×ª ×§×¦×¨×•×ª:
               * ×ª×§×¦×™×‘ ×§×¦×ª ×’×‘×•×” ×™×•×ª×¨?
               * ×¤×—×•×ª ×—×“×¨×™×?
               * ×‘×œ×™ ×—× ×™×”?
             - ×ª×©××œ ××™×–×” ×§×¨×™×˜×¨×™×•×Ÿ ×”×•× ×”×›×™ ×’×ž×™×© ×‘×•
             - ×ª×”×™×” ××•×¤×˜×™×ž×™ ××‘×œ ×§×¦×¨!
             - ×ž×§×¡×™×ž×•× 2 ××™×ž×•×’'×™× ×‘×”×•×“×¢×”!
             - ×”×©××œ×” ×‘×‘×•×œ×“: **×”×©××œ×”**
             - ×¢×‘×¨×™×ª ×‘×œ×‘×“!
             
             ×–×›×•×¨: ×§×¦×¨ ×•×—×‘×¨×•×ª×™, ×œ× ××¨×•×š ×•×ž×•×¨×›×‘!"""
        
        return prompt
    
    def generate_raw_response(self, prompt: str) -> str:
        """Generate raw AI response without any stage-specific formatting"""
        try:
            self._ensure_initialized()
            
            logger.info("Generating raw AI response")
            
            # Configure generation settings for more creative and human-like responses
            generation_config = types.GenerateContentConfig(
                temperature=1.2,
                top_p=0.95,
                top_k=40,
                max_output_tokens=400,
                safety_settings=[
                    types.SafetySetting(
                        category="HARM_CATEGORY_HARASSMENT",
                        threshold="BLOCK_ONLY_HIGH"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_HATE_SPEECH", 
                        threshold="BLOCK_ONLY_HIGH"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_SEXUALLY_EXPLICIT",
                        threshold="BLOCK_ONLY_HIGH"
                    ),
                    types.SafetySetting(
                        category="HARM_CATEGORY_DANGEROUS_CONTENT",
                        threshold="BLOCK_ONLY_HIGH"
                    ),
                ],
# Note: ThinkingConfig may not be available in this version
                # thinking_config disabled for now
            )
            
            response = self.client.models.generate_content(
                model=self.model_name,
                contents=prompt,
                config=generation_config
            )
            
            result = ""
            if hasattr(response, 'text') and response.text:
                result = response.text.strip()
            elif hasattr(response, 'candidates') and response.candidates:
                for candidate in response.candidates:
                    if hasattr(candidate, 'content') and hasattr(candidate.content, 'parts'):
                        for part in candidate.content.parts:
                            if hasattr(part, 'text') and part.text:
                                result += part.text
                result = result.strip()
            
            logger.info(f"Raw AI response generated, length: {len(result)}")
            return result
            
        except Exception as e:
            logger.error(f"Error generating raw AI response: {e}")
            return ""


# Global Gemini service instance
gemini_service = GeminiService()